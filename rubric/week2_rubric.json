{
  "rubric_metadata": {
    "rubric_name": "Week 2: The Automaton Auditor Peer Evaluation",
    "grading_target": "Week 2 Auditor Repository & Architectural Report",
    "version": "4.0.0"
  },
  "dimensions": [
    {
      "id": "development_progress",
      "name": "Development Progress",
      "target_artifact": "github_repo",
      "forensic_instruction": "Evaluate how much of the complete Week 2 deliverable the peer has implemented, as evidenced by the repository's git history and the functional state of the codebase. This is measured against the full final spec: state definitions, detective tools, judge nodes, Chief Justice synthesis, complete graph wiring, infrastructure, and audit reports. This criterion focuses on delivery completeness and the quality of the engineering process visible in git history, not report quality or theoretical depth.",
      "levels": {
        "Complete System": {
          "score": 35,
          "description": "Atomic, meaningful commits telling a clear development narrative across all phases. The full pipeline executes end-to-end: repo URL input through detectives (parallel fan-out/fan-in), through judges (parallel with distinct personas and structured output), through Chief Justice (deterministic conflict resolution), to a rendered Markdown audit report. All required files are present and substantive: state definitions with reducers, sandboxed tools, detective nodes, judge nodes with .with_structured_output(), justice node with hardcoded rules, complete graph with conditional edges, infrastructure files, and generated audit reports in audit/. The system has been run against at least the peer's own repo, producing a real report. Nothing structurally missing from the final deliverable spec. Any minor remaining gaps are documented with a clear rationale."
        },
        "Partial Pipeline": {
          "score": 21,
          "description": "More than 3 commits showing recognizable progression. Core infrastructure is in place: src/state.py with Pydantic/TypedDict models, src/tools/ with sandboxed repo cloning and PDF parsing, src/nodes/detectives.py producing structured Evidence objects, and src/graph.py wiring the detective layer with fan-out/fan-in. pyproject.toml, .env.example, and README.md are present. The detective pipeline runs end-to-end against a target repo. Missing: The judicial layer is incomplete or missing. Judge nodes may exist but lack distinct persona prompts, structured output enforcement, or parallel execution. The Chief Justice synthesis engine is absent or is just an LLM prompt without deterministic rules. Audit reports are not yet generated or are incomplete. The system works partially but cannot produce a full audit report from input to output."
        },
        "Superficial": {
          "score": 7,
          "description": "Repo exists with some code files. A single init or bulk upload commit contains all artifacts at once. Some deliverables exist but are stubs, placeholders, or clearly non-functional. State definitions may use plain dicts instead of Pydantic/TypedDict. No graph wiring or only a skeleton that does not execute. Missing: Iterative git history. Functional state management. Working detective or judge nodes. Any evidence of the code being tested against a real repo. The codebase does not reflect sustained engineering effort."
        },
        "Absent": {
          "score": 0,
          "description": "No repository exists, or the repo is empty / contains only boilerplate. No evidence of any development work."
        }
      }
    },
    {
      "id": "feedback_implementation",
      "name": "Feedback Implementation",
      "target_artifact": "github_repo",
      "forensic_instruction": "Evaluate whether the peer acted on feedback received during the development period -- from your direct communication, your code review, or from audit reports generated by your agent. This measures the peer's ability to iterate based on external input, which is the core of the MinMax loop. This criterion does not assess whether the feedback itself was correct. It assesses whether the peer engaged with received feedback and made visible, traceable changes as a result. If no feedback was exchanged, score 0 (No Exchange) and exclude from the weighted total.",
      "levels": {
        "Fully Integrated": {
          "score": 20,
          "description": "The peer addressed all substantive feedback points with traceable commits. Where feedback could not be fully addressed, the peer explicitly documented the deferral with a rationale (in commit messages, communication, or the report). Agent-generated audit findings were used to drive concrete architectural improvements, not just cosmetic fixes. The peer's response demonstrates comprehension of the feedback, not mechanical application. Evidence of the MinMax loop: feedback received led to improvements in both the peer's code and their understanding of what their own agent should detect. Nothing left unaddressed without documented reasoning."
        },
        "Selective": {
          "score": 12,
          "description": "The peer acknowledged feedback and addressed some of the points raised. Concrete changes are visible in git history following the feedback (e.g., commits fixing flagged issues, adding missing files, restructuring modules). Agent-generated findings, if shared, led to at least one concrete code change. Missing: Not all substantive feedback was addressed. The peer may have fixed surface-level issues (typos, missing files) while leaving deeper structural feedback unaddressed (e.g., linear graph wiring, missing reducers, persona collusion in judges). No explanation for why certain feedback was deferred."
        },
        "Ignored": {
          "score": 4,
          "description": "Feedback was provided (you flagged specific issues: missing files, broken imports, structural gaps, agent-generated findings), but the repo shows no evidence of changes related to that feedback. No acknowledgment in commits, code, or communication. Missing: Any sign that feedback was received and considered. No commit addressing flagged issues. No message acknowledging the points raised. The codebase is unchanged in the areas feedback targeted."
        },
        "No Exchange": {
          "score": 0,
          "description": "No feedback was given or received. Exclude from weighted total. Do not penalize the peer if no feedback channel was established."
        }
      }
    },
    {
      "id": "proactive_communication",
      "name": "Proactive Communication",
      "target_artifact": "communication_logs",
      "forensic_instruction": "Evaluate whether the peer proactively communicated about their progress, blockers, design decisions, and coordination needs throughout the development period. This measures initiative in making the peer-grading collaboration functional. This criterion does not assess technical skill or communication style. It assesses whether the peer made the feedback loop possible by being reachable, forthcoming, and engaged.",
      "levels": {
        "Collaborative Driver": {
          "score": 20,
          "description": "The peer regularly shared progress, flagged blockers early, and actively sought your input on design decisions and implementation trade-offs. They shared their repo and intermediate work early and often, enabling multiple rounds of feedback. They engaged with your agent's output and discussed findings. Communication was distributed across the development period. The peer treated the collaboration as a continuous loop, not a one-time handoff. The collaboration was genuinely bidirectional and productive throughout."
        },
        "Engaged": {
          "score": 12,
          "description": "The peer initiated communication at least a few times to share progress, ask questions, flag blockers, or discuss design decisions. They responded to your messages with enough detail to enable useful feedback. Their repo was shared early enough for you to run your agent against it and provide findings before the deadline. Missing: Consistency across the full development period. Communication may have been clustered (only at the start or only near the deadline). The peer may not have actively sought your input on architectural decisions or shared intermediate results for early review."
        },
        "Reactive Only": {
          "score": 4,
          "description": "The peer responded when you initiated contact, but never initiated communication themselves. Responses were minimal (e.g., ok, will do, done) without substantive detail. No sharing of progress or blockers without being prompted. Missing: Self-initiated updates. No proactive sharing of work-in-progress, no early sharing of the repo for review, no heads-up about delays or design decisions. You had to drive every interaction."
        },
        "Absent": {
          "score": 0,
          "description": "No communication from the peer at any point. No response to your messages. Unreachable."
        }
      }
    },
    {
      "id": "agent_feedback_relevance",
      "name": "Agent Feedback Relevance",
      "target_artifact": "audit_report",
      "forensic_instruction": "Evaluate whether YOUR auditor agent produced relevant, actionable, forensic-quality feedback when run against the peer's repository. This is a dual assessment: it tests whether the peer's repo is structured enough to be machine-auditable AND whether your agent is capable of generating genuine signal. If your agent crashes or produces no output, score 0 or 1 regardless of the peer's repo state. If the peer's repo is empty, your agent should handle that gracefully and report the absence -- that itself is valid forensic output. Be honest: if your agent failed to produce useful output, score accordingly. Inflating your own agent's performance undermines the adversarial improvement process.",
      "levels": {
        "Full Forensic Audit": {
          "score": 25,
          "description": "Your agent produced a complete audit report following the Automaton Auditor spec. Detectives collected structured evidence: git history analysis with progression assessment, AST-based code structure verification (not just regex), file existence with content analysis, and cross-referencing between report claims and repo reality. Judges (or equivalent analysis) provided multi-perspective evaluation with distinct viewpoints. The output includes a synthesized verdict with criterion-level scores, cited evidence, dissent where judges disagreed, and a specific remediation plan with file-level instructions. The feedback is detailed enough for the peer to take direct action. The agent demonstrates the full detective-judicial-synthesis pipeline against real input, producing output that matches the AuditReport structure."
        },
        "Targeted Analysis": {
          "score": 15,
          "description": "Your agent produced feedback referencing actual files and structures in the peer's repo. It correctly identified present deliverables and flagged genuinely missing components. Evidence objects contain real file paths with accurate existence checks. The output is clearly specific to this repo. Detective-layer output (git history analysis, state definition checks, file structure verification) is functional and grounded. Missing: Depth beyond file-level checks. The agent may verify file existence but not analyze code structure (e.g., whether the graph is truly parallel vs. linear, whether reducers are used, whether judge prompts are distinct). Judicial-layer analysis may be absent or shallow. No synthesized verdict with conflict resolution. The agent operates as a file checker, not a forensic auditor."
        },
        "Generic Noise": {
          "score": 5,
          "description": "Your agent ran but produced generic, non-specific output. Feedback is not tied to actual files or structures in the peer's repo. Output reads like boilerplate (Code quality is acceptable) or contains hallucinated file paths. The same output could have been generated for any repo. Evidence objects, if present, lack real file paths or accurate existence checks. Missing: Specificity. No file-level references matching actual repo contents. No distinction between present and missing artifacts. No structured Evidence objects grounded in real analysis."
        },
        "No Output": {
          "score": 0,
          "description": "Your agent could not run against the peer's repo. It crashed, produced no output, or was never executed. No feedback artifact exists. No evidence the agent was pointed at the peer's repo."
        }
      }
    }
  ]
}

